


# 1. import pandas and numpy libraries to perform mathematical functions
import numpy as np
import pandas as pd

# Note: The following code *can* be used to suppress warnings (though, best just to fix them!).
# import warnings
# warnings.filterwarnings('ignore')





# 2. assign cleaned .csv file to "advertising_data" variable
# Read given CSV file, and view some sample records
advertising_data = pd.read_csv("my_company_data.csv")

# display firat and last 5 records
advertising_data





# 3. print number of rows and columns
advertising_data.shape


# 4. print dataframe info (Note: also, indicated null values, which, if present, would need to be remedied.)
advertising_data.info()


# 5. print dataframe statistics summary
advertising_data.describe()

# format entire dataframe to two decimal places
pd.options.display.float_format = "{:,.2f}".format

print(advertising_data.describe())


# 6. Display pairwise correlations of *all* columns in dataframe.
advertising_data.corr().head()

# Note: "Perfect" correlations (1.0) with same attributes (e.g., "TV" and "TV").
# Note: High correlation between "TV" (ads) and "Sales"--much less for "Radio"!


# 7. import matplotlib and seaborn libraries to visualize data
# Note: # Seaborn provides more visualization patterns, with less syntax than matplotlib
import matplotlib.pyplot as plt
import seaborn as sns


# 8. visualize data for correlations using pairplot(). y=DV(s), x=IV(s)
# pairplot(): Plot pairwise relationships in dataset.
# https://seaborn.pydata.org/generated/seaborn.pairplot.html
# Note: scatter plot good when comparing two numeric variables, like here!
sns.pairplot(advertising_data, x_vars=['TV', 'Radio', 'Newspaper'], y_vars='Sales', height=4, aspect=1, kind='scatter')


# 9. Display one attribute's correlation ("Sales") to *all* other columns in dataframe, sorted in descending order by Sales.
advertising_data.corr()[['Sales']].sort_values(by='Sales', ascending=False)


# 10. Focus on correlation between "TV" (ads) and "Sales."
sns.relplot(data=advertising_data, x='TV', y='Sales')

# Note: Seaborn relational plot (relplot) visualizes how variables relate to each other within a dataset.
# Note: When looking for correlations, see if a line can be drawn through as many datapoints as possible.


# 11. Visually display correlations using Seaborn's heatmap() function.
# https://seaborn.pydata.org/generated/seaborn.heatmap.html
sns.heatmap(advertising_data.corr(), cmap="YlGnBu", annot = True)
plt.show()

# Note: "annot" property set to "True" so that r-values are displayed in each cell.
# Here, display color bar. To turn off: cbar=False (default True).


# 12. Visually condense correlations of one variable to other variables.
sns.heatmap(data=advertising_data.corr()[['Sales']].sort_values(by='Sales', ascending=False), annot=True, cmap='YlGnBu', cbar=False, fmt=f'.2f')

# Note: "annot" property set to "True" so that r-values are displayed in each cell.
# Here, hide the color bar here (cbar=False), as it is self-explanatory.
# Also note, no vmin/vmax properties used--which automatically determines min/max values from data (also, provides greater color contrast).





# 13. Identify x (IV) and y (DV)
x = advertising_data['TV']
y = advertising_data['Sales']


# 14. Create train and test datasets
# a. split variables into training and testing sets
# b. build model using training set, then run model on testing set

# Note: Training dataset used to *fit* the model, and test dataset is used to *evaluate* tthe model.
# Training data is the biggest subset of *original* dataset--used to train or fit the model.
# Test dataset is another subset of *original* data, independent of training dataset--validates model's accuracy.
# https://www.javatpoint.com/train-and-test-datasets-in-machine-learning

# split variables into train and test datasets into 7:3 ratio, respectively, by importing train_test_split
# Translation: 70% of observations for training, and remaining 30% for testing
# train_test_split():
# https://realpython.com/train-test-split-python-data/

# random_state: controls randomization during splitting (default value None).
# random_state value not important--can be any non-negative integer.
# Generally, *only* used to make tests reproducible--provides fixed seed value.
# https://numpy.org/doc/stable/reference/random/legacy.html#numpy.random.RandomState
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.7, test_size = 0.3, random_state = 100)

# Note: To date, there's no clear cut agreement on data split ratios, only common practices.
# https://onlinelibrary.wiley.com/doi/10.1002/sam.11583
# https://www.baeldung.com/cs/train-test-datasets-ratio





# 15. Display x training dataset (IVs)
x_train


# 16. Display y training dataset
y_train





# 17. Display shape for x train and test datasets, *before* adding column
print(x_train.shape)
print(x_test.shape)

# Note: Array "shape" is number of elements in each dimension. (Here, 1 dimension array.)





# 18. add additional column to train and test datasets
# By "reshaping" we can add or remove dimensions.
# https://www.w3schools.com/python/numpy/numpy_array_reshape.asp

# reshape() concept: first, raveling array (using given index order),
# then inserting/deleting elements from raveled array into new array, using same index order for raveling.
# https://numpy.org/doc/stable/reference/generated/numpy.reshape.html
x_train = x_train.values.reshape(-1, 1)
x_test = x_test.values.reshape(-1, 1)

# 19. display shape for x train and test data, * after* adding column (Now, 2 dimension array.)
print(x_train.shape)
print(x_test.shape)





# 20. fit regression line to plot
from sklearn.linear_model import LinearRegression

# Note: Can create linear regression model object (instantiation) and fit in two steps, or at the same time
# 1. Instantiate linear regression model object
# lm = LinearRegression()

# 2. fit model using fit() method
# lm.fit(x_train, y_train)

# Or...both at the same time...
lm = LinearRegression().fit(x_train, y_train)

# Note: fit() method accepts training dataset and fits regression line to dataset





# print model corefficients

# 21. print intercept value (aka the "constant"): Represents mean value of DV when all the IV(s) in model are equal to zero.
# Note: Also known as "y intercept." Simply, the value at which the fitted line (aka "regression line") crosses the y-axis.
# https://www.statology.org/intercept-in-regression/#:~:text=The%20intercept%20(sometimes%20called%20the,model%20are%20%equal%20to%20zero.
print("Intercept :", lm.intercept_)

# 22. print slope value: Indicates how much DV expected to change, as IV(s) increse(s)/decrease(s).
# https://www.dummies.com/article/academics-the-arts/math/statistics/how-to-interpret-a-regression-line-169717/
print('Slope :', lm.coef_)


# 23. simple linear regression formula from above values
# Y = mX + b
# Y = DV, X = IV, m = estimated slope, b = estimated intercept

# Note: parentheses not needed.
# Sales = (0.054 * TV) + 6.948





# 24. predict y_value
y_train_predict = lm.predict(x_train)
y_test_predict = lm.predict(x_test)

from sklearn.metrics import r2_score

# 25. print and compare r2 values of both train and test data
print(r2_score(y_train, y_train_predict))
print(r2_score(y_test, y_test_predict))

# Note (r2): Statistical measure of how well regression line approximates actual data
# Represents the proportion of variance for a DV that's explained by IV(s) in a regression model
# https://www.rasgoml.com/feature-engineering-tutorials/how-to-calculate-r-2-with-scikit-learn
# https://towardsdatascience.com/simple-linear-regression-model-using-python-machine-learning-eab7924d18b4

# Bottom-line: r2 values on test data within 5% of r2 values on training data. Model looks good!





# 26. Plot data and linear regression model fit.
# Note: Goal of seaborn is to help find and emphasize patterns through quick and easy visualizations.
# Ther are *MANY* ways to find relationships and determine patterns. Provided here are *some* ways of doing so.
# regplot(): https://seaborn.pydata.org/generated/seaborn.regplot.html

# Note: Seaborn is not itself a package for statistical analysis--for that, use scikit-learn, or statsmodels.
# https://seaborn.pydata.org/tutorial/regression.html#regression-tutorial

# Note: Regression lines indicate a linear relationship between DV(s) on y-axis and IV(s) on x-axis.
sns.regplot(data=advertising_data, x='TV', y='Sales', ci=95, scatter_kws={'s':5}, line_kws={"lw":1, 'color':'orange'})

# scatter_kws changes dot size (avoids overplotting and/or aesthetics). line_kws changes line color and width
# https://stackoverflow.com/questions/53257382/what-do-scatter-kws-and-line-kws-do-in-seaborn-lmplot
# https://eeob-biodata.github.io/BCB546X-python/05-seaborn-viz/

# ci (confidence interval used for regression estimate--None recommended)
# https://seaborn.pydata.org/generated/seaborn.lmplot.html

# *However*, here, confidence interval of 95% used for demo purposes.
# Note: Size of confidence interval for regression estimate--will be drawn using translucent bands around regression line.

# Confidence interval: a range around a measurement that indicates the measurement's precision.





# 27. Residual plot: used to plot residual values after plotting linear regression model.
# What is a residual plot? Helps to identify regression model fit.
# Note: "Residuals" are simply differences between DV test values and DV predicted values.
# Horizontal line indicates where predicted and actual values are the same.
# Dots above the line indicate underestimating--that is, predicted sales too low for TV ads.
# https://datogy.io/seaborn-residplot/

# Lowess (aka "Loess") line stands for "Locally weighted scatterplot smoothing."
# https://www.statisticshowto.com/lowess-smoothing/
# Creates smooth line to help better understand relationship between variables and trends.
# line_kws: color lowess line.
sns.residplot(data=advertising_data, x='TV', y='Sales', scatter_kws={'s':5}, lowess=True, line_kws={'color':'green'})



