


# 1. necessary libraries
# pandas and numpy libraries to perform mathematical functions
# matplotlib and seaborn libraries to visualize data
# linearRegression: models relationships between DVs, and given set of IVs
# train_test_split: splits dataset into training and test datasets

# Note: Seaborn provides more visualization patterns, with less syntax than matplotlib

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split





# 2. assign cleaned .csv file to "fish" variable
# Read given CSV file, and view some sample records
fish = pd.read_csv('fish.csv')

# display first and last 5 records
fish

# fish.hear() # display first 5 records
# fish.tail() # display last 5 records





# 3. Rename cols to differentiate "length" properties, then display 1st 5 records
fish.rename(columns={'Length1':'VerticalLength',
                     'Length2':'DiagonalLength',
                     'Length3':'CrossLength'}, inplace=True)

fish.head() # display first 5 records





# 4. print number of rows and columns
fish.shape


# 5. print dataframe info (Note: also, indicates null values, which, if present, would need to be remedied.)
fish.info()


# 6. print dataframe statistics summary
fish_describe = fish.describe()

# format entire dataframe to two decimal places
pd.options.display.float_format = "{:,.2f}".format

print(fish_describe)





# 7. Display pairwise correlations of *all* columns in dataframe.
fish.corr(numeric_only = True).head()

# Note: "Perfect" correlations (1.0) with same attributes (e.g., "Weight" and "Weight").
# Note: High correlations among *all* lengths, as well as "Width" and "Height" attributes.


# 8. visualize data for correlations using pairplot(). y=DV(s), x=IV(s)
# pairplot(): Plot pairwise relationships in dataset.
# https://seaborn.pydata.org/generated/seaborn.pairplot.html
# Note: scatter plot good when comparing two numeric variables, like here!
sns.pairplot(fish, x_vars=['Height', 'Width', 'VerticalLength'], y_vars="Weight", height=4, aspect=1, kind='scatter')
plt.show()


# 9. Custom visualizations
# Color palettes:
# https://seaborn.pydata.org/generated/seaborn.color_palette.html
# https://seaborn.pydata.org/tutorial/color_palettes.html

# display all colors from current default color cycle
sns.color_palette()


# 10. display color palette referenced by name
sns.color_palette("pastel")


# 11. display color values as hex codes:
print(sns.color_palette("pastel").as_hex())


# 12. Custom plotting
# https://seaborn.pydata.org/generated/seaborn.pairplot.html
# https://python-charts.com/correlation/pairs-plot-seaborn/
# https://python-charts.com/correlation/scatter-plot-group-seaborn/#customization

# extra code to suppress warnings
import warnings
warnings.filterwarnings('ignore', category=FutureWarning, module='seaborn._oldcore')

sns.pairplot(fish, hue="Species", diag_kind="hist", markers=["o", "s", "D", "*", "<", "p", "v" ], palette = "husl")


# 13. Display one attribute's correlation ("Weight") to *all* other columns in dataframe, sorted in descending ordery by Weight.
fish.corr(numeric_only=True)[('Weight')].sort_values(ascending=False)



# 14. Visually display correlations using Seaborn's heatmap() function.
# https://seaborn.pydata.org/generated/seaborn.heatmap.html

ax=sns.heatmap(data=fish.corr(numeric_only = True), annot=True, cmap='viridis')
ax.tick_params(axis='both', rotation=45, labelsize=8, labelcolor='blue', color='green') # customize ticks and labels

# choosing colormaps: https://matplotlib.org/stable/tutorials/colors/colormaps.html
# https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.tick_params.html


# 15. Visually condense correlations of one variable to other variables.
sns.heatmap(data=fish.corr(numeric_only=True)[['Weight']].sort_values(by='Weight', ascending=False), annot=True, cmap='PuBuGn', fmt='.2f')

# Note: "annot" property set to "True" so that r-values are displayed in each cell.
# Here, display color bar.
# Also note, no vmin/vmax properties used--which automatically determines min/max values from data (also, provides greater color contrast





# 16. Focus on one species: Bream
bream = fish.query('Species == "Bream"')


# 17. Identify X (IVs) and y (DV)
x = bream[['Height', 'Width', 'VerticalLength']]
y = bream['Weight']

# Note: Combination of 'Height', 'Width', 'VerticalLength' returns a higher correlation score than individual attributes by themselves (below).
# Compare multiple regression model "x" above, against simple regression model, that is, using each of the following attributes individually:

# x = bream[('VerticalLength')]
# x = bream[('Wdith')]
# x = bream[('Height')]

# *BE SURE* when comparing "x" values, only *one* should be uncommented!
# See #22 for model.score results!


# 18. Create train and test datasets
# a. split variables into training and testing sets
# b. build model using training set, then run model on testing sets

# Note: Training dataset used to *fit* the model, and test dataset is used to *evaluate* the model.
# Training data is the biggest subset of *original* dataset--used to train or fit the model.
# Test dataset is another (smaller) subset of *original* data, independent of training dataset--validates model's accuracy.
# Test dataset is another (smaller) subset of *original* data, independent of training dataset--validates model's accuracy.
# https://www.javatpoint.com/train-and-test-datasets-in-machine-learning

# split variables into train and test datasets into 75:25 ratio, respectively, by importing train_test_split
# Translation: 75% of observations for training, and remaining 25% for testing

# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.75, test_size = 0.25, random_state = 100)


# 19. Display x training dataset (IVs)
x_train # Note: Height, Width, and Lengths in centimeters


# 20. Display y training dataset (DV)
y_train/454 # convert weight in grams to pounds





# 21. fit regression line to plot
model = LinearRegression().fit(x_train, y_train)

# Note: fit() method accepts training dataset and fits regression line to dataset.


# 22. display R2 correlation values--validates model through correlation score
# score() function: accepts test dataset and returns R2 correlation value--i.e., percent of change in DV attributes to IV(s).
# higher score indicates better fit!
# https://scipy-lectures.org/packages/scikit-learn/index.html
model.score(x_test, y_test)

# Note: Underneath the hood, model.score = r2_score(y_actual, y_pred). See Assignment 4
# Automates prediction of data using x_test, and compares it with y_test--no need to manually derive y_pred.


# 23. predict weight and display predicted values based on IVs
# predict() method accepts x values from test dataset and returns predicated y values.
# predicts labels of data values, based on training model
y_predicted = model.predict(x_test)

# create "predicted" DataFrame with "PredictedWeight" column using y_predicted values
# Note: DataFrames are two-dimensional data structures, which indexes along with their associated values.
predicted = pd.DataFrame(y_predicted, columns = ['PredictedWeight'])

# display data structure type and "predicted" weight DataFrame values (Note: Index values assigned upon DataFrame creation.)
# type(predicted) # pandas.core.frame.DataFrame
predicted


# 24. display data structure type and values
# type(x_test) # pandas.core.frame.DataFrame
x_test # IVs


# 25. display data structure type and values
# type(y_test) # pandas.core.frame.DataFrame
y_test # DV


# 26. Join and display all columns, including PredictedWeight column
# pandas.DataFrame.join(): join multiple DataFrame objects by index at once by passing a list.
# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html
# Note: "drop" parameter removes old index values being added as a column
final = predicted.join([x_test.reset_index(drop=True),
                        y_test.reset_index(drop=True)])

# reset_index: Resets index values back to default values (0, 1, 2, etc.)
# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html

# display PredictedWeight and actual weight (DVs), with IVs
# Note: Very close predictions!
final





# 27. calculate and display residual values
# Note: "Residuals" are simply differences between DV test values and DV predicted values.
# Residual = Actual - Predicted
# positive values indicate prediction too low, negative values indicate prediction too high
final['Residual'] = final.Weight - final.PredictedWeight

final


# 28. Plot residuals
# KDE (Kernel Density Estimate) plot: Estimates data distribution
# Visual representation of underlying distribution of data--helps to understand shape and spread of data, and identify unusual outliers.
# Produces small continuous curve (also called kernel) for every individual data point along an axis.
# Scale of "density" asix depends on data values.

# https://seaborn.pydata.org/generated/seaborn.kdeplot.html
# https://datogy.io/seaborn-kdeplot/
# https://www.tutorialspoint.com/what-is-the-purpose-of-a-density-plot-or-kde-plot

# Note: Here, most residuals w/in +100 and -100 of zero, and closely centered on zero (0)--indicating a decent model!
# Plot reveals that outliers affecting regression are on negative side of curve.
sns.kdeplot(data=final, x="Residual")

# Note: "Ideal" KDE plot displays a bell-shaped curve centered over, on the X-axis, with most of the datapoints close to zero (0).



