


# Note: *After* installing NLTK (if not installed already) *must download NLTK datasets (corpus).
# Some important datasets: stpwords, guntenberg, framenet_v15, and others.

import pandas as pd # data analysis and manipulation
import nltk # Natural Language Toolkit - Language processing

# extra step - download stopwords for block 20
nltk.download('stopwords')

# Sentence Tokenizer: divides text into list of sentences (must be trained before it can be used)
# nltk.download('punkt') # after downloading comment out

# word tokenizer
from nltk.tokenize import word_tokenize

# nltk.download('stopwords') # only if needed, then comment out
# use to identify stop words - common words carrying little information (see below)
from nltk.corpus import stopwords

# use for tagging words with their pars of speech (POS) (e.g., nouns, verbs, etc.)
# nltk.download('averaged_perceptron_tagger') # after downloading comment out

# use for sentiment analysis - analyze positive/negative emotion of text (see below)
from nltk.sentiment import SentimentIntensityAnalyzer
# nltk.download('vader_lexicon') # after downloading comment out

# required to split data into train and test sets, where feature variables are given as input in method
from sklearn.model_selection import train_test_split

# CountVectorizer() converts collection of text documents into matrix.
from sklearn.feature_extraction.text import CountVectorizer

#classifies document based on counts it finds of multiple keywords
from sklearn.naive_bayes import MultinomialNB # import naive bayes

# used for confusion matrix in classification problems to assess errors in model
from sklearn import metrics

# determines accuracy classification score
from sklearn.metrics import accuracy_score

# library for creating static, animated, and interactive visualizations in Python
import matplotlib.pyplot as plt





# Load reviews into DataFrame
# Note: pipe (|) used instead of commas, as commas occur in reviews, and # indicates indexed column

df = pd.read_csv('GuitarReviews2out.txt', sep='|', index_col='#')

rows = df.shape[0] # num rows
cols = df.shape[1] # num col

# display number of rows/cols
print(rows)
print(cols)


df.head() # display first 5 reviews


df.iloc[0].review # display first review (from review column)





# combine all reviews from DataFrame into list for data manipulation/analysis
allTextList = df.review.to_list()

# used only for comparison
print(allTextList) # display list


allText = ' '.join(allTextList) # join elements of List with space
print(allText) # Note: elements no longer separated by commas, or include single quotations marks (')


# Tokenizers divide strings into lists of substrings
# resource: https://www.nltk.org/api/nltk.tokenize.html
# example: find words and punctuation in a string
# parse: tokenize text
tokens = nltk.word_tokenize(allText)

# print(tokens) # display all tokens
tokens[:10] # print only first 10 tokens


# determine word frequency
# Note: FreqDist() captures number of times each outcome of experiment hsa occurred
# https://www.nltk.org/api/nltk.probability.FreqDist.html
wordFrequency = nltk.FreqDist(tokens)
wordFrequency # display word frequency


# visualize/plot word frequency
wordFrequency.plot(30)





# keep tokens with Letters, using list comprehension
# Note: if necessary, review list comprehensions:
# https://www.w3schools.com/python/python_lists_comprehension.asp
alpha_words = [token for token in tokens if token.isalpha()]

alpha_words[:10] # print first 10 tokens w/letters


# cast alpha words into lower case, using list comprehension
lower_case_words = [word.lower() for word in alpha_words]

lower_case_words[:10] # print first 10 tokens w/Letters in lower case


# find stop words using NLTK stopwords package
# stop words: common words carrying little information
# explained:
# https://www.opinosis-analytics.com/knowledge-base/stop-words-exmplained/
# https://medium.com/@yashj302/stopwords-nlp-python-4aa57dc492af
# examples: "the," "is," "in," "for," "where, "when," "to," "at,"...
# NLTK's list of english stopwords: https://gist.github.com/sebleier/554280

# get NLTK English stopwords
stopWords = stopwords.words('english')

type(stopWords) # print stopWords type


len(stopWords) # print number of stop words


stopWords[:10] # display first 10 NLTK English stop words


# Remove stop words from Lower case words
lower_case_no_stop_words = [word for word in lower_case_words if word not in stopWords]

#display first 10 tokens w/Letters in Lower case, and parts of speech, *after* removing NLTK English stop words
lower_case_no_stop_words[:10]


# determine lower-case words w/no stop words frequency
wordFrequency = nltk.FreqDist(lower_case_no_stop_words)
wordFrequency # display frequency


# visualize/plot word frequency
wordFrequency.plot(30)


# stemming: remove morphological affixes from words, leaving only word stem (algorithm for suffix stripping)
# examples: playing = play, likes/likely/Liked = Like
# use Porter Stemmer (strip word suffixes)
porterStemmer = nltk.PorterStemmer()
stemmed_words = [porterStemmer.stem(word) for word in lower_case_no_stop_words]

# type(stemmed_words)
stemmed_words[:10] # print first 10 stemmed words


# Add part of speech to each token
# reference: https://www.nltk.org/book/ch05.html
wordsWithTags = nltk.pos_tag(tokens)
wordsWithTags[:10] # display first 10 tokens and their part of speech


# include only nouncs (tags beginning with N)
nouns = [word for (word, tag) in wordsWithTags if tag.startswith('N')]
nouns[:8]


# determine noun frequency
wordFrequency = nltk.FreqDist(nouns)
wordFrequency


# visualize/plot noun frequency
wordFrequency.plot(30)





# Sentiment Analysis:
# used to analyze positive/negative emotion of text (determine polarity of text: positive, negative, or neutral)
# https://www.nltk.org/howto/sentiment.html
# https://medium.com/@rslavanayageetha/vader-a-comprehensive-guide-to-sentiment-analysis-in-python-c4f1868b0d2e

# initialize SentimentIntensityAnalyzer object
analyzer = SentimentIntensityAnalyzer()


# analyze first review (from review column)
review1 = df.iloc[0].review
review1


# polarity_scores() method: returns dictionary of sentiment scores
# dictionary contains four key/value pairs: neg, neu, pos, and compound
# i.e., how negative (0-1), how neutral (0-1), how positive (0-1), as well as a compound score between -1 to 1
# compound: composite score of overall positive or negative sentiment (e.g., 0.9646 is very positive!)

# calculate polarity scores for first review
analyzer.polarity_scores(review1)


# loop through each review using polarity_scores() function
# display index, compound (composite) score, formatted to two decimal places, and review title
# Note: sixth review most positive, and ninth review only negative review
compoundList = []
for index, row in df.iterrows():
    text = row.review
    scores = analyzer.polarity_scores(text)
    compound = scores['compound']
    print(format(index, '2d'), format(compound, '6.2f'), row.title)
    compoundList.append(compound)


# more concisely, use DataFrame apply() method
# define function that calculates and returns compound VADER score
def compoundScore(text):
    scores = analyzer.polarity_scores(text)
    return scores['compound']

# apply analyzer on all reviews in DataFrame and display
# Note: apply() method passes function as an argument, and applies it on every single value of Pandas series

# apply compoundScore() function to "review" column in DataFrame, and create new DF column "compound"
df['compound'] = df['review'].apply(compoundScore)
df # display entire dataFrame


# or just display, index, title, and compound score
print(df[['title', 'compound']])





# Load data file of emails into DataFrame. Note: one line per email, pipe delimited
df = pd.read_csv('emails2.txt', usecols=['isSpam', 'Message'], sep='|')

rows = df.shape[0] # num rows
cols = df.shape[1] # num cols

# display number of rows/cols
print(rows)
print(cols)


df.head() # display first 5 e-mails


# review spam vs. nonspam emails (1=spam, 0=nonspam)
df.isSpam.value_counts()


# review (part of) first email (nonspam)
df.iloc[0].Message[:160] # display first 160 chars. of Message col.


# create function to remove nonletters
def remove_non_letters(text):
    alist = [c if c.isalpha() else ' ' for c in text]
    return ''.join(alist)

# iterate over Message col. using apply() method, and create new col.
df['NonLettersRemoved'] = df['Message'].apply(remove_non_letters)


# display first 160 chars. of Message col.
df.iloc[0].NonLettersRemoved[:160]


# tokenize e-mails
# create lambda function to tokenize filtered e-mail messages
tokenizer = lambda text: word_tokenize(text)
df['NonLettersRemoved'] = df['NonLettersRemoved'].apply(tokenizer)


# Note: tokenizer separates tokens wtih commas!
df['NonLettersRemoved'][:10] # display tokens for first 10 e-mails w/nonLetters removed


# stemming: remove morphological affixes from words, leaving onl yword stem (algorithm for suffix stripping)
# exapmles: playing = play, likes/likely/liked = like
# use Porter Stemmer (strip word suffixes)
stemmer = lambda words: [ porterStemmer.stem(word) for word in words ]
df['NonLettersRemoved'] = df['NonLettersRemoved'].apply(stemmer)


# display stemmed tokens for first 10 e-mails w/nonLetters removed
df['NonLettersRemoved'][:10]


# create Lambda function to rejoin tokenized e-mail messages
rejoiner = lambda words: ' '.join(words)
df['NonLettersRemoved'] = df['NonLettersRemoved'].apply(rejoiner)


# compare initial and transformed text for first 5 (nonspam) messages
df.head()


# compare initial and transformed text for last 5 (spam) messages
df.tail()





# dependent variable: isSpam (studied var.)
# independent variable: NonLettersRemoved (manipulated var.)

# split data into 25% "test" data and 75% "train" data
# Note: "generally," 25/75 is how data is split into test/train data sets

# returns four results (all Pandas "Series" data type):
# train_text and test_text: contain e-mail text
# train_labels and test_labels: contain binary values from isSpam column
train_text, test_text, train_labels, test_labels = \
 train_test_split(df.NonLettersRemoved, df.isSpam, test_size=0.25, random_state=1)


train_labels


# CountVectorizer(): Converts collection of text documents into matrix of token counts.
# rows represent documennts, and cols represent tokens (i.e., words or n-grams).
# counts occurrences of each token in each document.
# Note: "n-gram" is collection of n successive items in a text document--may include words, numbers, symbols, and punctuation.
# Resource: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html

# use CountVectorizer() to determine word freq. for each e-mail
# build "bag of words" (bow) features vectorizer and get features

# min_df=1: tracks words occurring at least once
# ngram_range=(1,1): finds single words, rather than word combinations
bow_vectorizer = CountVectorizer(min_df=1, ngram_range=(1,1))





# fit_transform(): used on training data
# counts occurrences of each word in each e-mail
bow_train_features = bow_vectorizer.fit_transform(train_text)


# transform(): used on test data
# transform(): use same mean and variance as calculated from training data to transform test data.
# Bottom-line: parameters learned by model using training data helps to transform test data.
bow_test_features = bow_vectorizer.transform(test_text)


# multinational Naive Bayes classifier: suitable for classification with discrete features (e.g., word counts for text classification).
# probobilistic classifier calculates probability distribution of text data
# multinomial distribution: used to find probilities in experiments, where there are more than two outcomes.
# Resource: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html

model = MultinomialNB()


# fit(): trains machine learning model on dataset.
# fit() method: takes in dataset (typically, 2D array or matrix), and a set of labels, then fits model to data.
# multinomialNB fit() method: expects x and y input.
# x: training vectors (i.e., training data)
# y: target values (i.e., labels, targets, or classes)

# Note: train model using training data, then predict using new data (i.e., test data, below). 
# fit() method: determines probabilities of individual words occurring in nonspam vs spam e-mails.
model.fit(bow_train_features, train_labels)


# predict nonspam vs spam e-mails using model and test data
predictions = model.predict(bow_test_features)


# number of emails in test data
len(test_labels)


# number of emails in training data
len(train_labels)


# Evaluating model's predictions:
# Compare actual spam/nonspam e-mails with model's prediction of spam/nonspam e-mails
test_results = \
    pd.DataFrame({'actual':test_labels.tolist(), 'predict':list(predictions)})
test_results


# display *all* rows where model was incorrect (index value indicates raw position)
# Note: only four rows!
test_results[test_results.actual != test_results.predict]


# calculate accuracy score for set of predicted labels against true labels
# resource: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html
accuracy_score(test_results.actual, test_results.predict)

# display as percentage (note: 94% accuracy!)
print('Accuracy {:.1%}'.format(accuracy_score(test_results.actual, test_results.predict)))


# also, can check accuracy using confusion matrix
# creates table to assess where errors occurred in model
# rows represent classes outcomes should have been
# columns represent predictions made
# table displays which predictions were wrong

# import "metrics" to use confusion matrix function on "actual" and "predicted" values
# rows represent actual classes that outcomes should have been
# columns represent predictions made
# Using table is an easy way to see which predictions are wrong!
# Generic syntax: confusion_matrix = metrics.confusion_matrix(actual, predicted)

confusion_matrix = metrics.confusion_matrix(test_results.actual, test_results.predict)

confusion_matrix


cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0, 1])

cm_display.plot()
plt.show()

# Confusion Matrix creates four quadrants:
# True Negatives (TN) (Top-Left Quadrant): Prediction no, true value no
# False Positives (FP) (Top-Right Quadrant): Predictionyes, true value no
# False Negatives (FN) (Bottom-Left Quadrant): Prediction no, true value yes
# True Positives (TP) (Bottom-Right Quadrant): Prediction yes, true value yes


# interpretation:
# row1: Model correctly categorized **nonspam** e-mails in 33 of 36 cases (91.7%), "specificity"
# row2: Model correctly categorized **spam** e-mails in 33 out of 34 cases (97.1%), "sensitivity"





# Accuracy measures how often model is correct
# Calculation: (True Positive + True Negative) / Total Predictions
# Example: Accuracy = metrics.accuracy_score(actual, predicted)

Accuracy = metrics.accuracy_score(test_results.actual, test_results.predict)


# Precision: Of positives predicted, what percentage is *truly* positive?
# Note: Precision does not evaluate correctly predicted negative cases:

# Calculation: True Positive / (True Positive + False Positive)
# Example: Precision = metrics.precision_score(actual, predicted)

Precision = metrics.precision_score(test_results.actual, test_results.predict)


# Sensitivity (aka Recall):
# Of all positive cases, what percentage are *predicted* positive?
# Measures how well model predicts something is positive.

# Translation: Looks at true positives and false negatives (which are positives that have been incorrectly predicted as negative).

# Calculation: True Positive / (True Positive + False Negative)
# Example: Sensitivity_recall = metrics.recall_score(actual, predicted)

Sensitivity_recall = metrics.recall_score(test_results.actual, test_results.predict)


# Specificity:
# How well model predicts negative results.
# Similar to sensitivity, but looks at it from perspective of negative results.

# Calculation: True Negative / (True Negative + False Positive)

# Note: Since it is opposite of Sensitivity/Recall, use recall_score function, taking opposite position label:
# Example: Specificity = metrics.recall_score(actual, predicted, pos_label=0)

Specificity = metrics.recall_score(test_results.actual, test_results.predict, pos_label=0)


# F-score: "Harmonic mean" of precision and sensitivity.
# Considers both false positive and false negative cases--good for imbalanced datasets.
# Note: Score does not take into consideration True Negative values.

# Calculation: 2 * ((Precision * Sensitivity) / (Precision + Sensitivity))

# Example: F1_score = metrics.f1_score(actual, predicted)

F1_score = metrics.f1_score(test_results.actual, test_results.predict)


# all calculations: print dictionary (Python dictionaries use curly braces {}), that is key: value pairs
print({"Accuracy":Accuracy,"Precision":Precision,"Sensitivity_recall":Sensitivity_recall,"Specificity":Specificity,"F1_score":F1_score})


# Or, to format nicely! :)
# my_dictionary = key:value pairs
my_dictionary = {"Accuracy":Accuracy,"Precision":Precision,"Sensitivity_recall":Sensitivity_recall,"Specificity":Specificity,"F1_score":F1_score}

# Note: "0" and "1" indicate field order--that is, key=0 and value=1
# Note: '<' Forces field to be left-aligned, within available space (default for most objects)
# Resource: https://docs.python.org/2/library/string.html#string-formatting

print("\n".join("{0: <16}\t{1:.2f}".format(k, v) for k, v in my_dictionary.items()))



