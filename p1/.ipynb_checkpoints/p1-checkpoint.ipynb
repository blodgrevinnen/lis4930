{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec39b55c-8d05-4b81-b8a8-d435cd24c5e7",
   "metadata": {},
   "source": [
    "# Project 1\n",
    "**Developer:** Tanner Morlan\n",
    "\n",
    "**Course:** LIS4930\n",
    "\n",
    "**Program Requirements:**\n",
    "\n",
    "1. Import necessary packages\n",
    "2. Review data\n",
    "3. Prepare data for analysis\n",
    "4. Filter data\n",
    "5. Display product review sentiment analysis\n",
    "6. Create prediction analysis\n",
    "\n",
    "### Part 1: Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdbb9c2-c21c-4f18-b666-799d57c08edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: *After* installing NLTK (if not installed already) *must download NLTK datasets (corpus).\n",
    "# Some important datasets: stpwords, guntenberg, framenet_v15, and others.\n",
    "\n",
    "import pandas as pd # data analysis and manipulation\n",
    "import nltk # Natural Language Toolkit - Language processing\n",
    "\n",
    "# extra step - download stopwords for block 20\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sentence Tokenizer: divides text into list of sentences (must be trained before it can be used)\n",
    "# nltk.download('punkt') # after downloading comment out\n",
    "\n",
    "# word tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# nltk.download('stopwords') # only if needed, then comment out\n",
    "# use to identify stop words - common words carrying little information (see below)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# use for tagging words with their pars of speech (POS) (e.g., nouns, verbs, etc.)\n",
    "# nltk.download('averaged_perceptron_tagger') # after downloading comment out\n",
    "\n",
    "# use for sentiment analysis - analyze positive/negative emotion of text (see below)\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "# nltk.download('vader_lexicon') # after downloading comment out\n",
    "\n",
    "# required to split data into train and test sets, where feature variables are given as input in method\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# CountVectorizer() converts collection of text documents into matrix.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#classifies document based on counts it finds of multiple keywords\n",
    "from sklearn.naive_bayes import MultinomialNB # import naive bayes\n",
    "\n",
    "# used for confusion matrix in classification problems to assess errors in model\n",
    "from sklearn import metrics\n",
    "\n",
    "# determines accuracy classification score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# library for creating static, animated, and interactive visualizations in Python\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c523c007-ed24-4ebb-94ff-edcf62ed8c83",
   "metadata": {},
   "source": [
    "### Part 2: Load and review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475f9b16-cba3-4877-884d-fa5b915ce2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reviews into DataFrame\n",
    "# Note: pipe (|) used instead of commas, as commas occur in reviews, and # indicates indexed column\n",
    "\n",
    "df = pd.read_csv('GuitarReviews2out.txt', sep='|', index_col='#')\n",
    "\n",
    "rows = df.shape[0] # num rows\n",
    "cols = df.shape[1] # num col\n",
    "\n",
    "# display number of rows/cols\n",
    "print(rows)\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd9a6c5-9d34-49ca-80cd-1e43b95c4627",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() # display first 5 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0addfc7d-153c-4eba-b006-9f854d42eea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0].review # display first review (from review column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b422becc-df52-458d-816a-852c9b7ffbc8",
   "metadata": {},
   "source": [
    "### Part 3: Prepare, tokenize, and visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae4c70f-1143-4712-b23a-ba7bbd8de1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all reviews from DataFrame into list for data manipulation/analysis\n",
    "allTextList = df.review.to_list()\n",
    "\n",
    "# used only for comparison\n",
    "print(allTextList) # display list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7efe3c-5c6d-464a-a18e-f1820e14a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "allText = ' '.join(allTextList) # join elements of List with space\n",
    "print(allText) # Note: elements no longer separated by commas, or include single quotations marks (')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e0a286-d70f-456f-bf4f-fc12a3440ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizers divide strings into lists of substrings\n",
    "# resource: https://www.nltk.org/api/nltk.tokenize.html\n",
    "# example: find words and punctuation in a string\n",
    "# parse: tokenize text\n",
    "tokens = nltk.word_tokenize(allText)\n",
    "\n",
    "# print(tokens) # display all tokens\n",
    "tokens[:10] # print only first 10 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d396a5b-f94d-4c8a-b1ea-50f77742e4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine word frequency\n",
    "# Note: FreqDist() captures number of times each outcome of experiment hsa occurred\n",
    "# https://www.nltk.org/api/nltk.probability.FreqDist.html\n",
    "wordFrequency = nltk.FreqDist(tokens)\n",
    "wordFrequency # display word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dc954e-b362-4119-a9d2-2a7fb3e9bc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize/plot word frequency\n",
    "wordFrequency.plot(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5762dd9e-ba0f-4510-805b-b3c510ca3644",
   "metadata": {},
   "source": [
    "### Part 4: Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5353ab7e-9c06-4b76-9587-2674fa4a3397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep tokens with Letters, using list comprehension\n",
    "# Note: if necessary, review list comprehensions:\n",
    "# https://www.w3schools.com/python/python_lists_comprehension.asp\n",
    "alpha_words = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "alpha_words[:10] # print first 10 tokens w/letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a514f515-9600-422c-b1c9-a30bccdfeb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast alpha words into lower case, using list comprehension\n",
    "lower_case_words = [word.lower() for word in alpha_words]\n",
    "\n",
    "lower_case_words[:10] # print first 10 tokens w/Letters in lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee58e8a0-aee6-425c-bb8e-964076eddf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find stop words using NLTK stopwords package\n",
    "# stop words: common words carrying little information\n",
    "# explained:\n",
    "# https://www.opinosis-analytics.com/knowledge-base/stop-words-exmplained/\n",
    "# https://medium.com/@yashj302/stopwords-nlp-python-4aa57dc492af\n",
    "# examples: \"the,\" \"is,\" \"in,\" \"for,\" \"where, \"when,\" \"to,\" \"at,\"...\n",
    "# NLTK's list of english stopwords: https://gist.github.com/sebleier/554280\n",
    "\n",
    "# get NLTK English stopwords\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "type(stopWords) # print stopWords type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405deb7b-2237-4bd6-b55f-39301497adca",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stopWords) # print number of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a10395-619a-4b35-bd8e-6032ad0c304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords[:10] # display first 10 NLTK English stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61897c6-a6bc-4c61-be99-15d4c273bba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words from Lower case words\n",
    "lower_case_no_stop_words = [word for word in lower_case_words if word not in stopWords]\n",
    "\n",
    "#display first 10 tokens w/Letters in Lower case, and parts of speech, *after* removing NLTK English stop words\n",
    "lower_case_no_stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24309c39-eb81-4932-9f91-9d0be6798ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine lower-case words w/no stop words frequency\n",
    "wordFrequency = nltk.FreqDist(lower_case_no_stop_words)\n",
    "wordFrequency # display frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25f2917-a96f-4fbb-b650-7c97712e6424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize/plot word frequency\n",
    "wordFrequency.plot(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeb87c6-cb05-41df-9fc0-20dccd0d52b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming: remove morphological affixes from words, leaving only word stem (algorithm for suffix stripping)\n",
    "# examples: playing = play, likes/likely/Liked = Like\n",
    "# use Porter Stemmer (strip word suffixes)\n",
    "porterStemmer = nltk.PorterStemmer()\n",
    "stemmed_words = [porterStemmer.stem(word) for word in lower_case_no_stop_words]\n",
    "\n",
    "# type(stemmed_words)\n",
    "stemmed_words[:10] # print first 10 stemmed words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc77b59-090e-4096-84d2-efb54dcca613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add part of speech to each token\n",
    "# reference: https://www.nltk.org/book/ch05.html\n",
    "wordsWithTags = nltk.pos_tag(tokens)\n",
    "wordsWithTags[:10] # display first 10 tokens and their part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3231401d-b211-4dda-98a2-4f83e9e23fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# include only nouncs (tags beginning with N)\n",
    "nouns = [word for (word, tag) in wordsWithTags if tag.startswith('N')]\n",
    "nouns[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6272e25b-89b2-417f-b782-0b183828e2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine noun frequency\n",
    "wordFrequency = nltk.FreqDist(nouns)\n",
    "wordFrequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8b9cb6-e149-4fd3-a71d-0ca1152aad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize/plot noun frequency\n",
    "wordFrequency.plot(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1522e9-1aa2-4b50-93aa-fbb6bb33e04d",
   "metadata": {},
   "source": [
    "### Part 5: Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8027fcbe-6c88-4b7d-9e98-111fdec943f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis:\n",
    "# used to analyze positive/negative emotion of text (determine polarity of text: positive, negative, or neutral)\n",
    "# https://www.nltk.org/howto/sentiment.html\n",
    "# https://medium.com/@rslavanayageetha/vader-a-comprehensive-guide-to-sentiment-analysis-in-python-c4f1868b0d2e\n",
    "\n",
    "# initialize SentimentIntensityAnalyzer object\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfbefd0-ef80-4b79-8041-8266ee610a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze first review (from review column)\n",
    "review1 = df.iloc[0].review\n",
    "review1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb12d011-795a-4a81-b75e-4fbcbad85594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# polarity_scores() method: returns dictionary of sentiment scores\n",
    "# dictionary contains four key/value pairs: neg, neu, pos, and compound\n",
    "# i.e., how negative (0-1), how neutral (0-1), how positive (0-1), as well as a compound score between -1 to 1\n",
    "# compound: composite score of overall positive or negative sentiment (e.g., 0.9646 is very positive!)\n",
    "\n",
    "# calculate polarity scores for first review\n",
    "analyzer.polarity_scores(review1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa06a4-5cf9-4ccc-a70c-0690a86279de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through each review using polarity_scores() function\n",
    "# display index, compound (composite) score, formatted to two decimal places, and review title\n",
    "# Note: sixth review most positive, and ninth review only negative review\n",
    "compoundList = []\n",
    "for index, row in df.iterrows():\n",
    "    text = row.review\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    compound = scores['compound']\n",
    "    print(format(index, '2d'), format(compound, '6.2f'), row.title)\n",
    "    compoundList.append(compound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e19290e-edcc-48ae-aa2f-7c3670fafbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more concisely, use DataFrame apply() method\n",
    "# define function that calculates and returns compound VADER score\n",
    "def compoundScore(text):\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    return scores['compound']\n",
    "\n",
    "# apply analyzer on all reviews in DataFrame and display\n",
    "# Note: apply() method passes function as an argument, and applies it on every single value of Pandas series\n",
    "\n",
    "# apply compoundScore() function to \"review\" column in DataFrame, and create new DF column \"compound\"\n",
    "df['compound'] = df['review'].apply(compoundScore)\n",
    "df # display entire dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfafff37-6366-43ba-9251-38e61dc8f853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or just display, index, title, and compound score\n",
    "print(df[['title', 'compound']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e58534-ca2f-48dc-a0e5-4ab563f753c2",
   "metadata": {},
   "source": [
    "### Part 6: Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c997270-5072-4dc3-8b2c-579fd27284aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data file of emails into DataFrame. Note: one line per email, pipe delimited\n",
    "df = pd.read_csv('emails2.txt', usecols=['isSpam', 'Message'], sep='|')\n",
    "\n",
    "rows = df.shape[0] # num rows\n",
    "cols = df.shape[1] # num cols\n",
    "\n",
    "# display number of rows/cols\n",
    "print(rows)\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90194d08-1f3e-47b9-89e1-c156bb78e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() # display first 5 e-mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4681ca2-7cd8-4066-a411-e028bb15f58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review spam vs. nonspam emails (1=spam, 0=nonspam)\n",
    "df.isSpam.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2085902a-ee77-46dd-8e7f-d04ab8812d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review (part of) first email (nonspam)\n",
    "df.iloc[0].Message[:160] # display first 160 chars. of Message col."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1a69df-ab9f-4829-9c51-00e8b7f35708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to remove nonletters\n",
    "def remove_non_letters(text):\n",
    "    alist = [c if c.isalpha() else ' ' for c in text]\n",
    "    return ''.join(alist)\n",
    "\n",
    "# iterate over Message col. using apply() method, and create new col.\n",
    "df['NonLettersRemoved'] = df['Message'].apply(remove_non_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac83195-2b61-43ed-bef1-3e1e91635a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display first 160 chars. of Message col.\n",
    "df.iloc[0].NonLettersRemoved[:160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c8948a-6316-4506-8e0a-e66567f8b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize e-mails\n",
    "# create lambda function to tokenize filtered e-mail messages\n",
    "tokenizer = lambda text: word_tokenize(text)\n",
    "df['NonLettersRemoved'] = df['NonLettersRemoved'].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a11f1de-67df-4022-acb1-1bba3b02f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: tokenizer separates tokens wtih commas!\n",
    "df['NonLettersRemoved'][:10] # display tokens for first 10 e-mails w/nonLetters removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc4960e-fe33-47dd-95ef-8b0ef394fe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming: remove morphological affixes from words, leaving onl yword stem (algorithm for suffix stripping)\n",
    "# exapmles: playing = play, likes/likely/liked = like\n",
    "# use Porter Stemmer (strip word suffixes)\n",
    "stemmer = lambda words: [ porterStemmer.stem(word) for word in words ]\n",
    "df['NonLettersRemoved'] = df['NonLettersRemoved'].apply(stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5ef6c4-8c17-4eda-9bd6-199d3e3cc19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display stemmed tokens for first 10 e-mails w/nonLetters removed\n",
    "df['NonLettersRemoved'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da452afd-5424-4d23-b1be-d63d7feddec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Lambda function to rejoin tokenized e-mail messages\n",
    "rejoiner = lambda words: ' '.join(words)\n",
    "df['NonLettersRemoved'] = df['NonLettersRemoved'].apply(rejoiner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0667823-9fd3-4b26-bf92-3c4033fb7ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare initial and transformed text for first 5 (nonspam) messages\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d075ee5-c7e1-4d7d-8e55-3e53119b0614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare initial and transformed text for last 5 (spam) messages\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211f55a9-251f-4746-85d9-648048c20938",
   "metadata": {},
   "source": [
    "### Part 7: Predictive Analysis\n",
    "\n",
    "#### Definitions\n",
    "1. **Dependent variables (also called)**: response, outcome/output, or target variables (respond to changes in (an)other variable(s)\n",
    "2. **Independent variables (also called)**: predictor, input, regressor, or explanatory variable(s) (predict/explain changed values of dependent variable(s)\n",
    "\n",
    "*Dependent* variables (**output on y-axis**) are *always* the ones being studied--that is, whose variable(s) is/are being modified somehow!\n",
    "\n",
    "*Independent* variables (**input on x-axis**) are *always* the ones being manipulated, to study and compare the effects of the dependent variable(s).\n",
    "\n",
    "**Note**: The designationns *independent* and *dependent* variables are used to not imply \"cause and effect\" (as do \"predictor\" or \"explanatory\" terms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bc6ea6-73a1-4d33-a0cc-4a97373c56c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependent variable: isSpam (studied var.)\n",
    "# independent variable: NonLettersRemoved (manipulated var.)\n",
    "\n",
    "# split data into 25% \"test\" data and 75% \"train\" data\n",
    "# Note: \"generally,\" 25/75 is how data is split into test/train data sets\n",
    "\n",
    "# returns four results (all Pandas \"Series\" data type):\n",
    "# train_text and test_text: contain e-mail text\n",
    "# train_labels and test_labels: contain binary values from isSpam column\n",
    "train_text, test_text, train_labels, test_labels = \\\n",
    " train_test_split(df.NonLettersRemoved, df.isSpam, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e362826-1941-402c-9a05-376af5c927c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806b891-15cd-402e-8939-667b755add2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer(): Converts collection of text documents into matrix of token counts.\n",
    "# rows represent documennts, and cols represent tokens (i.e., words or n-grams).\n",
    "# counts occurrences of each token in each document.\n",
    "# Note: \"n-gram\" is collection of n successive items in a text document--may include words, numbers, symbols, and punctuation.\n",
    "# Resource: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "# use CountVectorizer() to determine word freq. for each e-mail\n",
    "# build \"bag of words\" (bow) features vectorizer and get features\n",
    "\n",
    "# min_df=1: tracks words occurring at least once\n",
    "# ngram_range=(1,1): finds single words, rather than word combinations\n",
    "bow_vectorizer = CountVectorizer(min_df=1, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84161ff0-fdd5-4500-b8fa-4f7a42e1b86e",
   "metadata": {},
   "source": [
    "#### **fit() vs. transform() vs. fit_transform() methods:**\n",
    "\n",
    "- **fit()**: calculates mean and variance of each of the features in data.\n",
    "- **transform()**: transforms all features using respective mean and variance.\n",
    "- **fit_transform()**: used on training data to scale training data and also learn scaling parameters of data.\n",
    "\n",
    "Model built will learn mean and variance of features of training set; learned parameters then used to scale test data\n",
    "\n",
    "**Resource**: https://towardsdatascience.com/what-and-why-behind-fit-transform-vs-transform-in-scikit-learn-78f915cf96fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bfb3e5-b65b-4e36-9727-39f910a762eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_transform(): used on training data\n",
    "# counts occurrences of each word in each e-mail\n",
    "bow_train_features = bow_vectorizer.fit_transform(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e00d266-e7db-48cf-a05f-d6936f144fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform(): used on test data\n",
    "# transform(): use same mean and variance as calculated from training data to transform test data.\n",
    "# Bottom-line: parameters learned by model using training data helps to transform test data.\n",
    "bow_test_features = bow_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffa1c71-15d4-44a9-91e9-01b184d8f73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multinational Naive Bayes classifier: suitable for classification with discrete features (e.g., word counts for text classification).\n",
    "# probobilistic classifier calculates probability distribution of text data\n",
    "# multinomial distribution: used to find probilities in experiments, where there are more than two outcomes.\n",
    "# Resource: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "\n",
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503263f0-2853-4e97-9458-09be5b1f52dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit(): trains machine learning model on dataset.\n",
    "# fit() method: takes in dataset (typically, 2D array or matrix), and a set of labels, then fits model to data.\n",
    "# multinomialNB fit() method: expects x and y input.\n",
    "# x: training vectors (i.e., training data)\n",
    "# y: target values (i.e., labels, targets, or classes)\n",
    "\n",
    "# Note: train model using training data, then predict using new data (i.e., test data, below). \n",
    "# fit() method: determines probabilities of individual words occurring in nonspam vs spam e-mails.\n",
    "model.fit(bow_train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b4d3b8-225d-4f6e-86a4-65f244d545ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict nonspam vs spam e-mails using model and test data\n",
    "predictions = model.predict(bow_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1726b63-5850-4c65-bbc8-6b0f2556def4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of emails in test data\n",
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54efb80-e8b2-48f3-937e-528b48dcfcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of emails in training data\n",
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cc7a15-6355-4376-a4d6-b8d680447cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating model's predictions:\n",
    "# Compare actual spam/nonspam e-mails with model's prediction of spam/nonspam e-mails\n",
    "test_results = \\\n",
    "    pd.DataFrame({'actual':test_labels.tolist(), 'predict':list(predictions)})\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d777c96-afc5-465d-a91e-1096cbbc2f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display *all* rows where model was incorrect (index value indicates raw position)\n",
    "# Note: only four rows!\n",
    "test_results[test_results.actual != test_results.predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecd442d-5ad9-4dd8-b80b-12866606d911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy score for set of predicted labels against true labels\n",
    "# resource: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "accuracy_score(test_results.actual, test_results.predict)\n",
    "\n",
    "# display as percentage (note: 94% accuracy!)\n",
    "print('Accuracy {:.1%}'.format(accuracy_score(test_results.actual, test_results.predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e860f6-8d68-4c79-b128-057c96a6696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also, can check accuracy using confusion matrix\n",
    "# creates table to assess where errors occurred in model\n",
    "# rows represent classes outcomes should have been\n",
    "# columns represent predictions made\n",
    "# table displays which predictions were wrong\n",
    "\n",
    "# import \"metrics\" to use confusion matrix function on \"actual\" and \"predicted\" values\n",
    "# rows represent actual classes that outcomes should have been\n",
    "# columns represent predictions made\n",
    "# Using table is an easy way to see which predictions are wrong!\n",
    "# Generic syntax: confusion_matrix = metrics.confusion_matrix(actual, predicted)\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(test_results.actual, test_results.predict)\n",
    "\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e233c85-1596-4dee-b8ff-bea69435f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0, 1])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix creates four quadrants:\n",
    "# True Negatives (TN) (Top-Left Quadrant): Prediction no, true value no\n",
    "# False Positives (FP) (Top-Right Quadrant): Predictionyes, true value no\n",
    "# False Negatives (FN) (Bottom-Left Quadrant): Prediction no, true value yes\n",
    "# True Positives (TP) (Bottom-Right Quadrant): Prediction yes, true value yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9293c410-e6f6-485c-874b-0715c73920df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpretation:\n",
    "# row1: Model correctly categorized **nonspam** e-mails in 33 of 36 cases (91.7%), \"specificity\"\n",
    "# row2: Model correctly categorized **spam** e-mails in 33 out of 34 cases (97.1%), \"sensitivity\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffabf398-955f-434b-8540-d8ee49b61030",
   "metadata": {},
   "source": [
    "#### **Confusion Matrix**: Table compares predicted and actual values.\n",
    "\n",
    "||Predicted: No (not spam) | Predicted: Yes (spam) | Total: |\n",
    "|:----------------------|:------------------------:|:---------------------:|:------:|\n",
    "| **Actual: No (not spam)** | TN=33 | FP=3 | 36 |\n",
    "| **Actual: Yes (spam)** | FN=1 | TP=33 | 34 |\n",
    "| **Total** | 34 | 36 ||\n",
    "\n",
    "## Related Metrics:\n",
    "\n",
    "| Metric | Formula | Definition |\n",
    "|:-------|:-------:|:----------:|\n",
    "| **Accuracy** | (TP+TN)/(TP+TN+FP+FN) | Percentage of total items classified correctly |\n",
    "| **Precision** | TP/(TP+FP) | Positive predictions accuracy |\n",
    "| **Recall/Sensitivity** | TP/(TP+FN) | True positive rate (e.g., assess false positive rate) |\n",
    "| **Specificity** | TN/(TN+FP) | True negative rate (e.g., assess false negative rate) |\n",
    "| **F1 score** | 2TP/(2TP+FP+FN) | Weighted average of precision and recall/sensitivity |\n",
    "\n",
    "**Resources:** <br />\n",
    "https://machine-learning.paperspace.com/wiki/confusion-matrix <br />\n",
    "https://classeval.wordpress.com/introduction/basic-evaluation-measures/ <br />\n",
    "https://poojapawani.medium.com/what-is-confusion-matrix-accuracy-sensitivity-specificity-precision-recall-1091b4723714 <br />\n",
    "https://www.w3schools.com/python/python_ml_confusion_matrix.asp <br />\n",
    "\n",
    "### Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4740dd57-721b-4f90-b60a-126c20c8a87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy measures how often model is correct\n",
    "# Calculation: (True Positive + True Negative) / Total Predictions\n",
    "# Example: Accuracy = metrics.accuracy_score(actual, predicted)\n",
    "\n",
    "Accuracy = metrics.accuracy_score(test_results.actual, test_results.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be57fc39-6eb9-4a4b-9a6f-316b2d41f852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision: Of positives predicted, what percentage is *truly* positive?\n",
    "# Note: Precision does not evaluate correctly predicted negative cases:\n",
    "\n",
    "# Calculation: True Positive / (True Positive + False Positive)\n",
    "# Example: Precision = metrics.precision_score(actual, predicted)\n",
    "\n",
    "Precision = metrics.precision_score(test_results.actual, test_results.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3fe9bf-8541-4a51-9a93-00d7ba468e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity (aka Recall):\n",
    "# Of all positive cases, what percentage are *predicted* positive?\n",
    "# Measures how well model predicts something is positive.\n",
    "\n",
    "# Translation: Looks at true positives and false negatives (which are positives that have been incorrectly predicted as negative).\n",
    "\n",
    "# Calculation: True Positive / (True Positive + False Negative)\n",
    "# Example: Sensitivity_recall = metrics.recall_score(actual, predicted)\n",
    "\n",
    "Sensitivity_recall = metrics.recall_score(test_results.actual, test_results.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239d21b5-a4b6-41a9-9a9c-8817c6f95bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specificity:\n",
    "# How well model predicts negative results.\n",
    "# Similar to sensitivity, but looks at it from perspective of negative results.\n",
    "\n",
    "# Calculation: True Negative / (True Negative + False Positive)\n",
    "\n",
    "# Note: Since it is opposite of Sensitivity/Recall, use recall_score function, taking opposite position label:\n",
    "# Example: Specificity = metrics.recall_score(actual, predicted, pos_label=0)\n",
    "\n",
    "Specificity = metrics.recall_score(test_results.actual, test_results.predict, pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30733a3-04bf-4c23-8d83-18b2912ea271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F-score: \"Harmonic mean\" of precision and sensitivity.\n",
    "# Considers both false positive and false negative cases--good for imbalanced datasets.\n",
    "# Note: Score does not take into consideration True Negative values.\n",
    "\n",
    "# Calculation: 2 * ((Precision * Sensitivity) / (Precision + Sensitivity))\n",
    "\n",
    "# Example: F1_score = metrics.f1_score(actual, predicted)\n",
    "\n",
    "F1_score = metrics.f1_score(test_results.actual, test_results.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee27908c-e170-4921-aa2a-bf6edaa20195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all calculations: print dictionary (Python dictionaries use curly braces {}), that is key: value pairs\n",
    "print({\"Accuracy\":Accuracy,\"Precision\":Precision,\"Sensitivity_recall\":Sensitivity_recall,\"Specificity\":Specificity,\"F1_score\":F1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922a0417-113a-4c4e-b391-a6806293b868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or, to format nicely! :)\n",
    "# my_dictionary = key:value pairs\n",
    "my_dictionary = {\"Accuracy\":Accuracy,\"Precision\":Precision,\"Sensitivity_recall\":Sensitivity_recall,\"Specificity\":Specificity,\"F1_score\":F1_score}\n",
    "\n",
    "# Note: \"0\" and \"1\" indicate field order--that is, key=0 and value=1\n",
    "# Note: '<' Forces field to be left-aligned, within available space (default for most objects)\n",
    "# Resource: https://docs.python.org/2/library/string.html#string-formatting\n",
    "\n",
    "print(\"\\n\".join(\"{0: <16}\\t{1:.2f}\".format(k, v) for k, v in my_dictionary.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e63ea76-a7f2-4137-865a-02830e4ebf83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
